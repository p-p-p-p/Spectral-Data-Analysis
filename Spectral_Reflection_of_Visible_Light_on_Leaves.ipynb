{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[florence-sam](https://huggingface.co/spaces/SkalskiP/florence-sam)<br>\n",
        "[Florence-2](https://huggingface.co/spaces/gokaygokay/Florence-2)<br>\n",
        "[sam2](https://ai.meta.com/sam2/)<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/p-p-p-p/store/main/demo.png\">\n",
        "\n",
        "You can use Florence-2 to detect specific objects, such as a single leaf in an image, by passing a prompt like \"a single leaf.\" Florence-2 will identify and locate all instances of leaves in the image, which can then be passed to the SAM2 model. SAM2 will generate masks for each detected object, isolating them within the image.\n",
        "\n",
        "Once you have the masks, you can select the one that best matches your prompt (e.g., the first detected leaf) and use OpenCV to create a mask to isolate the leaf from the background. This will allow you to extract only that object from the original image, with the background being white. After that, you can perform spectral reflection of visible light on the extracted leaf to obtain a Reflectance vs. Wavelength graph.\n",
        "\n"
      ],
      "metadata": {
        "id": "GdsUIiT1Mhsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "szw6eNozEvO7"
      },
      "outputs": [],
      "source": [
        "#@title Install required packages and Auto - Restart Session [If you get any pop-up click on cancel]\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/spaces/SkalskiP/florence-sam\n",
        "!wget https://treeplantation.com/images/articles/tree-leaves.png\n",
        "%cd /content/florence-sam\n",
        "!pip install -r /content/florence-sam/requirements.txt\n",
        "!pip install gradio==4.36.1\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import time\n",
        "time.sleep(5)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title After seeing the red icon above run from here"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6x2DM-wbS5_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Play Audio to active the colab Notebook { display-mode: \"form\" }\n",
        "\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then run the cell below</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "vwevcPWPFiDV",
        "outputId": "19f826ef-0d0e-4b64-e993-e37f011291b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>Press play on the music player to keep the tab alive, then run the cell below</b><br/>\n",
              "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils\n",
        "%cd /content/florence-sam\n",
        "import os\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import spaces\n",
        "import supervision as sv\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.florence import load_florence_model, run_florence_inference, \\\n",
        "    FLORENCE_DETAILED_CAPTION_TASK, \\\n",
        "    FLORENCE_CAPTION_TO_PHRASE_GROUNDING_TASK, FLORENCE_OPEN_VOCABULARY_DETECTION_TASK\n",
        "from utils.modes import IMAGE_INFERENCE_MODES, IMAGE_OPEN_VOCABULARY_DETECTION_MODE\n",
        "from utils.sam import load_sam_image_model, run_sam_inference\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "# DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "FLORENCE_MODEL, FLORENCE_PROCESSOR = load_florence_model(device=DEVICE)\n",
        "SAM_IMAGE_MODEL = load_sam_image_model(device=DEVICE)\n",
        "COLORS = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700', '#32CD32', '#8A2BE2']\n",
        "COLOR_PALETTE = sv.ColorPalette.from_hex(COLORS)\n",
        "BOX_ANNOTATOR = sv.BoxAnnotator(color=COLOR_PALETTE, color_lookup=sv.ColorLookup.INDEX)\n",
        "LABEL_ANNOTATOR = sv.LabelAnnotator(\n",
        "    color=COLOR_PALETTE,\n",
        "    color_lookup=sv.ColorLookup.INDEX,\n",
        "    text_position=sv.Position.CENTER_OF_MASS,\n",
        "    text_color=sv.Color.from_hex(\"#000000\"),\n",
        "    border_radius=5\n",
        ")\n",
        "MASK_ANNOTATOR = sv.MaskAnnotator(\n",
        "    color=COLOR_PALETTE,\n",
        "    color_lookup=sv.ColorLookup.INDEX\n",
        ")\n",
        "\n",
        "\n",
        "def annotate_image(image, detections):\n",
        "    output_image = image.copy()\n",
        "    output_image = MASK_ANNOTATOR.annotate(output_image, detections)\n",
        "    output_image = BOX_ANNOTATOR.annotate(output_image, detections)\n",
        "    output_image = LABEL_ANNOTATOR.annotate(output_image, detections)\n",
        "    return output_image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "@torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16)\n",
        "def process_image(image_input, text_input,mode_dropdown=IMAGE_OPEN_VOCABULARY_DETECTION_MODE\n",
        ") -> Tuple[Optional[Image.Image], Optional[str]]:\n",
        "    if not image_input:\n",
        "        gr.Info(\"Please upload an image.\")\n",
        "        return None, None\n",
        "    text_input=text_input.lower()\n",
        "    if mode_dropdown == IMAGE_OPEN_VOCABULARY_DETECTION_MODE:\n",
        "        if not text_input:\n",
        "            gr.Info(\"Please enter a text prompt.\")\n",
        "            return None, None\n",
        "\n",
        "        texts = [prompt.strip() for prompt in text_input.split(\",\")]\n",
        "        detections_list = []\n",
        "        for text in texts:\n",
        "            _, result = run_florence_inference(\n",
        "                model=FLORENCE_MODEL,\n",
        "                processor=FLORENCE_PROCESSOR,\n",
        "                device=DEVICE,\n",
        "                image=image_input,\n",
        "                task=FLORENCE_OPEN_VOCABULARY_DETECTION_TASK,\n",
        "                text=text\n",
        "            )\n",
        "            detections = sv.Detections.from_lmm(\n",
        "                lmm=sv.LMM.FLORENCE_2,\n",
        "                result=result,\n",
        "                resolution_wh=image_input.size\n",
        "            )\n",
        "            detections = run_sam_inference(SAM_IMAGE_MODEL, image_input, detections)\n",
        "            detections_list.append(detections)\n",
        "\n",
        "        detections = sv.Detections.merge(detections_list)\n",
        "        detections = run_sam_inference(SAM_IMAGE_MODEL, image_input, detections)\n",
        "        # return annotate_image(image_input, detections), None\n",
        "        return detections\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "def mask_image(pillow_image, detections):\n",
        "  # Convert the Pillow image to OpenCV format (NumPy array)\n",
        "  opencv_image = np.array(pillow_image)\n",
        "  # Convert RGB to BGR (if needed, OpenCV uses BGR by default)\n",
        "  opencv_image = cv2.cvtColor(opencv_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "  # Extract the detection mask and bounding box\n",
        "  mask = list(detections[0])[0][1]\n",
        "  xyxy = list(detections[0])[0][0]\n",
        "  mask=mask.astype(np.uint8)\n",
        "  # Convert bounding box coordinates to integers\n",
        "  x_min, y_min, x_max, y_max = map(int, xyxy)\n",
        "  # cv2_imshow(opencv_image)\n",
        "\n",
        "  # Create a white background image of the same size as the cropped image\n",
        "  white_background = np.ones_like(opencv_image) * 255\n",
        "\n",
        "  # Apply the resized mask to the cropped image\n",
        "  masked_roi = cv2.bitwise_and(opencv_image, opencv_image, mask=mask)\n",
        "  # cv2.imwrite(\"/content/temp.png\",masked_roi)\n",
        "  # Place the masked ROI onto the white background\n",
        "  white_background = cv2.bitwise_or(white_background, white_background, mask=cv2.bitwise_not(mask))\n",
        "  white_background[mask == 1] = masked_roi[mask == 1]\n",
        "  rgb_image = cv2.cvtColor(white_background, cv2.COLOR_BGR2RGB)\n",
        "  return rgb_image\n",
        "  # rgb_image = cv2.cvtColor(white_background, cv2.COLOR_BGR2RGB)\n",
        "  # pillow_mask_image = Image.fromarray(rgb_image)\n",
        "  # return pillow_mask_image\n",
        "\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import uuid\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(\"/content/Dataset\"):\n",
        "    os.mkdir(\"/content/Dataset\")\n",
        "\n",
        "def get_file_name():\n",
        "    random_uuid = str(uuid.uuid4())[:10]\n",
        "    return f\"/content/Dataset/{random_uuid}\"\n",
        "\n",
        "def extract_rgb_values_with_numpy(img_array):\n",
        "    random_file_name = get_file_name()\n",
        "    json_filename = f\"{random_file_name}.json\"\n",
        "    graph_image_path = f\"{random_file_name}_mask.png\"\n",
        "    mask_image_path = f\"{random_file_name}.png\"\n",
        "\n",
        "    # Save the image\n",
        "    bgr_image = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(mask_image_path, bgr_image)\n",
        "\n",
        "    # Dictionary to store pixel information, reflectance, and wavelengths\n",
        "    store_data = {\n",
        "        \"pixel_info\": [],\n",
        "        \"reflectance_values\": [],\n",
        "        \"wavelength_values\": []\n",
        "    }\n",
        "\n",
        "    # Get the dimensions of the image\n",
        "    height, width, _ = img_array.shape\n",
        "\n",
        "    # Iterate through each pixel\n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            # Get the RGB values\n",
        "            r, g, b = map(int, img_array[y, x])\n",
        "\n",
        "            def my_map(x, in_min, in_max, out_min, out_max):\n",
        "                return int((x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min)\n",
        "\n",
        "            # Map 8 bits value of R,G,B (0 to 255) to their bandwidth of wavelength\n",
        "            R = my_map(r, 0, 255, 499, 700)  # rgb 0 = 499 nm, rgb 255 = 700\n",
        "            G = my_map(g, 0, 255, 440 , 580)  # rgb 0 = 440 nm, rgb 255 = 580\n",
        "            B = my_map(b, 0, 255, 380, 490)  # rgb 0 = 440 nm, rgb 255 = 490\n",
        "\n",
        "            # Mean of constituent wavelengths\n",
        "            wavelength = (R + G + B) / 3\n",
        "            wavelength_rounded = round(wavelength, 2)\n",
        "\n",
        "            # Calculate reflectance\n",
        "            reflectance = int(0.2125 * r + 0.7154 * g + 0.0721 * b)\n",
        "            r_percentage = (reflectance / 320) * 100\n",
        "            r_percentage_rounded = round(r_percentage, 2)\n",
        "\n",
        "            # Store pixel information, reflectance, and wavelength as a tuple\n",
        "            store_data[\"pixel_info\"].append(((x, y), r_percentage_rounded, wavelength_rounded))\n",
        "\n",
        "    # Sort the list based on wavelengths\n",
        "    sorted_info = sorted(store_data[\"pixel_info\"], key=lambda x: x[2])\n",
        "\n",
        "    # Collect reflectance and wavelength values for plotting\n",
        "    store_data[\"reflectance_values\"] = [reflectance for _, reflectance, _ in sorted_info]\n",
        "    store_data[\"wavelength_values\"] = [wavelength for _, _, wavelength in sorted_info]\n",
        "\n",
        "    # Plotting the Reflectance values\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    plt.plot(sorted(store_data[\"wavelength_values\"]), sorted(store_data[\"reflectance_values\"]), marker='o', label='Reflectance')\n",
        "    plt.xlabel('Wavelength (nm)')\n",
        "    plt.ylabel('Reflectance')\n",
        "    plt.title('Reflectance vs. Wavelength')\n",
        "    plt.xlim(380, 700)  # Set x-axis limits\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the plot to a file\n",
        "    plt.savefig(graph_image_path)  # Specify your desired filename and format\n",
        "    plt.close()  # Close the plot to free up memory\n",
        "\n",
        "    # Create a new dictionary excluding 'pixel_info'\n",
        "    reduced_data = {\n",
        "        \"reflectance_values\": store_data[\"reflectance_values\"],\n",
        "        \"wavelength_values\": store_data[\"wavelength_values\"]\n",
        "    }\n",
        "\n",
        "    # Save the reduced dictionary to a JSON file\n",
        "    # with open(json_filename, 'w') as json_file:\n",
        "    #     json.dump(reduced_data, json_file, indent=4)  # indent for pretty printing\n",
        "    return mask_image_path,graph_image_path\n",
        "# Example usage\n",
        "\n",
        "def gradio_image_process(pillow_image,prompt):\n",
        "  detections=process_image(pillow_image , prompt)\n",
        "  opencv_mask_image=mask_image(pillow_image, detections)\n",
        "  mask_image_path,graph_image_path=extract_rgb_values_with_numpy(opencv_mask_image)\n",
        "  return mask_image_path,graph_image_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "_ZkkERIsHPgU",
        "outputId": "6c700301-59e7-43d2-c2b7-a7e727f8a40b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/florence-sam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_path = '/content/tree-leaves.png'  # @param {type: \"string\"}\n",
        "prompt = 'a single leaf'  # @param {type: \"string\"}\n",
        "\n",
        "pillow_image = Image.open(image_path)\n",
        "if pillow_image.mode != 'RGB':\n",
        "    pillow_image = pillow_image.convert('RGB')\n",
        "gradio_image_process(pillow_image,prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "C5xNhiipGZvt",
        "outputId": "dd104e78-d3ce-4bc0-c0ad-31ed071ff981"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/Dataset/635655a5-e.png', '/content/Dataset/635655a5-e_mask.png')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradio API Interface\n",
        "\n",
        "import gradio as gr\n",
        "gradio_examples = [[\"/content/tree-leaves.png\",\"a single leaf\"]]\n",
        "gradio_input=[gr.Image(label=\"Upload or Capture Image\",type='pil',sources=['upload', 'webcam'],mirror_webcam=False),gr.Textbox(label=\"Text prompt\",value=\"a single leaf\")]\n",
        "gradio_output=[gr.Image(label=\"Detect leaf\"),gr.Image(label=\"Reflectance vs. Wavelength graph\")]\n",
        "gradio_interface = gr.Interface(fn=gradio_image_process, inputs=gradio_input,outputs=gradio_output , title=\"Spectral Reflection of Visible Light on Leaves\",examples=gradio_examples,cache_examples=True)\n",
        "gradio_interface.queue().launch(share=True,debug=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "cellView": "form",
        "id": "isEFQLHfIGaC",
        "outputId": "23c35840-5b02-4662-df3b-80fa3c732b0e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching examples at: '/content/florence-sam/gradio_cached_examples/73'\n",
            "Caching example 1/1\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://74d7697b3dd93339ae.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://74d7697b3dd93339ae.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}